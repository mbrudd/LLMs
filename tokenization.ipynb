{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f592fbb0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "_Tokenization_ is the process of converting a body of text into individual _tokens_, e.g., words and punctuation characters. This is the first step for most Natural Language Processing (NLP) tasks, including preparing data for training an LLM. Let's see how it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f17e49",
   "metadata": {},
   "source": [
    "## Some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc42f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test! Or is this not a test? Test it to be sure. :)\n",
      "This sample text has 61 characters.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test! Or is this not a test? Test it to be sure. :)\"\n",
    "print(text)\n",
    "print(f\"This sample text has {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39fa51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test!', 'Or', 'is', 'this', 'not', 'a', 'test?', 'Test', 'it', 'to', 'be', 'sure.', ':)']\n"
     ]
    }
   ],
   "source": [
    "print( text.split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2bd7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "  sep\n",
      "    The separator used to split the string.\n",
      "\n",
      "    When set to None (the default value), will split on any whitespace\n",
      "    character (including \\n \\r \\t \\f and spaces) and will discard\n",
      "    empty strings from the result.\n",
      "  maxsplit\n",
      "    Maximum number of splits.\n",
      "    -1 (the default value) means no limit.\n",
      "\n",
      "Splitting starts at the front of the string and works to the end.\n",
      "\n",
      "Note, str.split() is mainly useful for data that has been intentionally\n",
      "delimited.  With natural text that includes punctuation, consider using\n",
      "the regular expression module.\n",
      "\u001b[0;31mType:\u001b[0m      method_descriptor"
     ]
    }
   ],
   "source": [
    "str.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa28134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f735a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ')', '.', ':', '?', 'Or', 'Test', 'This', 'a', 'be', 'is', 'it', 'not', 'sure', 'test', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split( r'([.?!:()]|\\s)', text)\n",
    "tokens = [ item for item in tokens if item.split() ]\n",
    "tokens = sorted( list( set( tokens ) ) )\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb31ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('!', 0), (')', 1), ('.', 2), (':', 3), ('?', 4), ('Or', 5), ('Test', 6), ('This', 7), ('a', 8), ('be', 9), ('is', 10), ('it', 11), ('not', 12), ('sure', 13), ('test', 14), ('this', 15), ('to', 16)])\n"
     ]
    }
   ],
   "source": [
    "vocab = { token:index for index, token in enumerate( tokens )}\n",
    "print( vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "529bfc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "757ab7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Return an enumerate object.\n",
      "\n",
      "  iterable\n",
      "    an object supporting iteration\n",
      "\n",
      "The enumerate object yields pairs containing a count (from start, which\n",
      "defaults to zero) and a value yielded by the iterable argument.\n",
      "\n",
      "enumerate is useful for obtaining an indexed list:\n",
      "    (0, seq[0]), (1, seq[1]), (2, seq[2]), ...\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "enumerate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bb986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a577d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57edbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4320c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make training faster on a laptop, change context_length as shown:\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b5282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744511e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26be878",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eceda77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youlo recruited outfits1985 cycling pH Pace cancel Cec949\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d589e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you forward\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3366315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probs.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016d2404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.1785e-05, 1.6226e-05, 2.0078e-05,  ..., 1.7387e-05,\n",
       "          2.7340e-05, 2.4068e-05],\n",
       "         [1.1193e-05, 5.0324e-06, 2.2962e-05,  ..., 1.9476e-05,\n",
       "          2.3490e-05, 1.9991e-05],\n",
       "         [9.4089e-06, 7.7095e-06, 1.4245e-05,  ..., 1.8782e-05,\n",
       "          3.4639e-05, 1.8724e-05]],\n",
       "\n",
       "        [[1.6899e-05, 7.4190e-05, 2.1845e-05,  ..., 1.3892e-05,\n",
       "          2.2013e-05, 2.9669e-05],\n",
       "         [1.9167e-05, 1.4280e-05, 8.1801e-06,  ..., 1.9727e-05,\n",
       "          2.3493e-05, 1.5351e-05],\n",
       "         [1.2105e-05, 1.3187e-05, 2.6216e-05,  ..., 3.1856e-05,\n",
       "          2.6217e-05, 3.5807e-05]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02286211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[45376],\n",
      "         [23441],\n",
      "         [47511]],\n",
      "\n",
      "        [[22342],\n",
      "         [44427],\n",
      "         [ 2281]]])\n"
     ]
    }
   ],
   "source": [
    "# predicted tokens:\n",
    "token_ids = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c5fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: LOS cubeJar\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e208aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 2:  really like chocolate\n",
      "Outputs batch 2: Land033 phys\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n",
    "print(f\"Outputs batch 2: {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39e34eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3626,  6100,   345],\n",
       "        [ 1107,   588, 11311]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9e1274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([5.0002e-06, 1.2645e-05, 8.3261e-06])\n",
      "Text 2: tensor([1.2283e-05, 1.4727e-05, 1.4207e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probs_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6b54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions( sci_mode=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18344499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.2060, -11.2782, -11.6961, -11.3073, -11.1258, -11.1617])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b08df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.4625)\n",
      "tensor(11.4625)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)\n",
    "neg_avg_log_probs = avg_log_probs * -1\n",
    "print(neg_avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cecc539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d014ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2559, -0.0387,  0.1743,  ...,  0.0304,  0.4830,  0.3555],\n",
      "        [-0.4053, -1.2047,  0.3133,  ...,  0.1486,  0.3360,  0.1747],\n",
      "        [-0.5804, -0.7795, -0.1656,  ...,  0.1109,  0.7230,  0.1078],\n",
      "        [ 0.0076,  1.4870,  0.2643,  ..., -0.1884,  0.2720,  0.5704],\n",
      "        [ 0.1276, -0.1667, -0.7238,  ...,  0.1565,  0.3312, -0.0943],\n",
      "        [-0.3278, -0.2422,  0.4450,  ...,  0.6399,  0.4450,  0.7568]])\n",
      "tensor([ 3626,  6100,   345,  1107,   588, 11311])\n"
     ]
    }
   ],
   "source": [
    "print(logits_flat)\n",
    "print(targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4496e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4625)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da356f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(95085.6172)\n"
     ]
    }
   ],
   "source": [
    "# a more interpretable version of cross entropy --\n",
    "# this is basically the number of tokens the model considers for predicted output\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be5a53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT fifteen years ago, on a date late in August or early in September, a train drew up at Wilsthor\n"
     ]
    }
   ],
   "source": [
    "# use the short story from before for training:\n",
    "with open( \"humphreys.txt\", \"r\" ) as f:\n",
    "    text_data = f.read()\n",
    "print(text_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec615ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51a99344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate text into training and validation sets:\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "170dd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 12800\n",
      "Validation tokens: 1280\n",
      "All tokens: 14080\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb08b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddf190bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n",
      "Training loss: 11.012925834655762\n",
      "Validation loss: 11.05364958445231\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53faf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4661c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.751, Val loss 9.792\n",
      "Ep 1 (Step 000005): Train loss 8.250, Val loss 8.244\n",
      "Ep 1 (Step 000010): Train loss 7.125, Val loss 7.223\n",
      "Ep 1 (Step 000015): Train loss 6.369, Val loss 6.873\n",
      "Ep 1 (Step 000020): Train loss 6.206, Val loss 6.732\n",
      "Every effort moves you of the’, and the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of\n",
      "Ep 2 (Step 000025): Train loss 6.070, Val loss 6.817\n",
      "Ep 2 (Step 000030): Train loss 5.727, Val loss 6.790\n",
      "Ep 2 (Step 000035): Train loss 5.557, Val loss 6.659\n",
      "Ep 2 (Step 000040): Train loss 5.594, Val loss 6.590\n",
      "Ep 2 (Step 000045): Train loss 5.618, Val loss 6.596\n",
      "Every effort moves you he hadreys, and the maze.”                                        \n",
      "Ep 3 (Step 000050): Train loss 5.423, Val loss 6.505\n",
      "Ep 3 (Step 000055): Train loss 5.217, Val loss 6.500\n",
      "Ep 3 (Step 000060): Train loss 5.086, Val loss 6.566\n",
      "Ep 3 (Step 000065): Train loss 4.878, Val loss 6.453\n",
      "Ep 3 (Step 000070): Train loss 4.852, Val loss 6.459\n",
      "Every effort moves you”                                                \n",
      "Ep 4 (Step 000075): Train loss 4.770, Val loss 6.350\n",
      "Ep 4 (Step 000080): Train loss 4.625, Val loss 6.348\n",
      "Ep 4 (Step 000085): Train loss 4.309, Val loss 6.393\n",
      "Ep 4 (Step 000090): Train loss 4.068, Val loss 6.366\n",
      "Ep 4 (Step 000095): Train loss 4.230, Val loss 6.369\n",
      "Every effort moves you know, and the maze, and the house, and the maze, and a long at the house, and the house, and a great of a of the maze, and the maze. It was not, and of a maze, and the maze\n",
      "Ep 5 (Step 000100): Train loss 3.905, Val loss 6.338\n",
      "Ep 5 (Step 000105): Train loss 3.438, Val loss 6.356\n",
      "Ep 5 (Step 000110): Train loss 3.707, Val loss 6.402\n",
      "Ep 5 (Step 000115): Train loss 3.396, Val loss 6.396\n",
      "Ep 5 (Step 000120): Train loss 2.839, Val loss 6.434\n",
      "Every effort moves you” “I've to the house,” ” ” ” ” ” ” ” “I” ” ” ” \n",
      "Ep 6 (Step 000125): Train loss 2.926, Val loss 6.433\n",
      "Ep 6 (Step 000130): Train loss 2.589, Val loss 6.436\n",
      "Ep 6 (Step 000135): Train loss 2.500, Val loss 6.473\n",
      "Ep 6 (Step 000140): Train loss 2.378, Val loss 6.505\n",
      "Ep 6 (Step 000145): Train loss 2.211, Val loss 6.561\n",
      "Every effort moves you know, he would be all the nicer.    “No, I'm sorry to say I'm sure you’s very much.“It.“I“I’s kind about a“\n",
      "Ep 7 (Step 000150): Train loss 1.969, Val loss 6.584\n",
      "Ep 7 (Step 000155): Train loss 1.880, Val loss 6.618\n",
      "Ep 7 (Step 000160): Train loss 1.560, Val loss 6.641\n",
      "Ep 7 (Step 000165): Train loss 1.499, Val loss 6.679\n",
      "Ep 7 (Step 000170): Train loss 1.444, Val loss 6.709\n",
      "Every effort moves you on the roof almost be all the central circle and had not, and pointed out in the country; there was a basement, and a rather imposing flight of it was awaiting them. It was the first time. The one that it was at the shape\n",
      "Ep 8 (Step 000175): Train loss 1.200, Val loss 6.779\n",
      "Ep 8 (Step 000180): Train loss 1.067, Val loss 6.806\n",
      "Ep 8 (Step 000185): Train loss 0.982, Val loss 6.910\n",
      "Ep 8 (Step 000190): Train loss 0.827, Val loss 6.950\n",
      "Ep 8 (Step 000195): Train loss 0.813, Val loss 6.966\n",
      "Every effort moves you on a Man’s Body lying in the Roadway, and a maze.    “Really? Have you could git a clear view right through. Humphreys. He had not done much before an interruption came in the Heart\n",
      "Ep 9 (Step 000200): Train loss 0.761, Val loss 6.895\n",
      "Ep 9 (Step 000205): Train loss 0.654, Val loss 6.987\n",
      "Ep 9 (Step 000210): Train loss 0.641, Val loss 7.116\n",
      "Ep 9 (Step 000215): Train loss 0.472, Val loss 7.137\n",
      "Ep 9 (Step 000220): Train loss 0.492, Val loss 7.179\n",
      "Every effort moves you see it on a date late in August or early in the Bars of the Gate?’ But all would not do: the Man was set upon his Purpose: for it seems it was the common fireside Talk of that Country that at the Heart\n",
      "Ep 10 (Step 000225): Train loss 0.446, Val loss 7.259\n",
      "Ep 10 (Step 000230): Train loss 0.375, Val loss 7.329\n",
      "Ep 10 (Step 000235): Train loss 0.425, Val loss 7.307\n",
      "Ep 10 (Step 000240): Train loss 0.291, Val loss 7.339\n",
      "Ep 10 (Step 000245): Train loss 0.275, Val loss 7.402\n",
      "Every effort moves you see it just at all the pictures in the impression of the one that leads up to this very building. Above and below Draco were outlined various figures not unlike the pictures of the ordinary constellations, but not the centre, but at a nude man\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
